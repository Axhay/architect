## DynamoDB 

1. Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second.
Many of the world's fastest growing businesses such as Lyft, Airbnb, and Redfin as well as enterprises such as Samsung, Toyota, and Capital One depend on the scale and performance of DynamoDB to support their mission-critical workloads.
Hundreds of thousands of AWS customers have chosen DynamoDB as their key-value and document database for mobile, web, gaming, ad tech, IoT, and other applications that need low-latency data access at any scale. Create a new table for your application and let DynamoDB handle the rest.
2. Extreme Performance While DynamoDB offers consistent single-digit millisecond latency, DynamoDB + DAX takes performance to the next level with response times in microseconds for millions of requests per second for read-heavy workloads. With DAX, your applications remain fast and responsive, even when a popular event or news story drives unprecedented request volumes your way. No tuning required.
3. DAX handles millions of req per sec
4. Let’s assume you want to take a backup of one of your DynamoDB tables each day. A simple way to achieve this is to use an Amazon CloudWatch Events rule to trigger an AWS Lambda function daily. In this scenario, in your Lambda function you have the code required to call the dynamodb:CreateBackup API operation
5. You can tell the application to perform a strongly consistent read. This will force DynamoDB to return the most up to date information.
6. AWS Data pipeline is the web service you can use to automate the movement and transformation of data.  You can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. You define the parameters of your data transformations and AWS data Pipeline enforces the logic that you have setup.
7. Many companies consider migrating from relational databases like MySQL to Amazon DynamoDB, a fully managed, fast, highly scalable, and flexible NoSQL database service. For example, DynamoDB can increase or decrease capacity based on traffic, in accordance with business needs. The total cost of servicing can be optimized more easily than for the typical media-based RDBMS.
8. The 8KB is significant as it fits inside of the 400KB item size limit of DynamoDB and there is viable:
Items
Item Size
The maximum item size in DynamoDB is 400 KB
Other than this low latency retrieval, durable and 'cost is not a factor' all points to the DynamoDB answer here.
9. DynamoDB is typically useful for storing a large number of small records with single digit millisecond latency.
10. DynamoDB. Keywords: indexed data
11. "key = value" and "semi-structured" ==> NoSQL DB = DynamoDB
12. Amazon DynamoDB transactions simplify the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily.
13. Requirements: MOST cost-efficient change to support the increased traffic with minimal changes to the application
* Cost effective: only scale up when needed
* No change to application

## More:

1. Fast and flexible noSQL database service for all apps that need consistent, single-digit millisecond latency at any scale.
2. Fully managed DB and supports both document and key value data models.
3. Make it a great fit for mobile, web, gaming, ad-tech, IOT and many other apps.

## Basics:

1. Stored on SSD storage.
2. Spread across 3 geographically distinct data centers.
3. Eventual consistent reads (Default)
4. Strongly consistent reads.

## Eventual consistent reads:

1. Consistency across all copies of data reached in second.
2. Best read performance. Repeating read after short time returns updated data.

## Strongly consistent reads:

1. Returns a result that reflects all writes that received a successful response prior to the read.

