## DynamoDB 

1. Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second.
Many of the world's fastest growing businesses such as Lyft, Airbnb, and Redfin as well as enterprises such as Samsung, Toyota, and Capital One depend on the scale and performance of DynamoDB to support their mission-critical workloads.
Hundreds of thousands of AWS customers have chosen DynamoDB as their key-value and document database for mobile, web, gaming, ad tech, IoT, and other applications that need low-latency data access at any scale. Create a new table for your application and let DynamoDB handle the rest.
2. Extreme Performance While DynamoDB offers consistent single-digit millisecond latency, DynamoDB + DAX takes performance to the next level with response times in microseconds for millions of requests per second for read-heavy workloads. With DAX, your applications remain fast and responsive, even when a popular event or news story drives unprecedented request volumes your way. No tuning required.
3. DAX handles millions of req per sec
4. Let’s assume you want to take a backup of one of your DynamoDB tables each day. A simple way to achieve this is to use an Amazon CloudWatch Events rule to trigger an AWS Lambda function daily. In this scenario, in your Lambda function you have the code required to call the dynamodb:CreateBackup API operation
5. You can tell the application to perform a strongly consistent read. This will force DynamoDB to return the most up to date information.
6. AWS Data pipeline is the web service you can use to automate the movement and transformation of data.  You can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. You define the parameters of your data transformations and AWS data Pipeline enforces the logic that you have setup.
7. Many companies consider migrating from relational databases like MySQL to Amazon DynamoDB, a fully managed, fast, highly scalable, and flexible NoSQL database service. For example, DynamoDB can increase or decrease capacity based on traffic, in accordance with business needs. The total cost of servicing can be optimized more easily than for the typical media-based RDBMS.
8. The 8KB is significant as it fits inside of the 400KB item size limit of DynamoDB and there is viable:
Items
Item Size
The maximum item size in DynamoDB is 400 KB
Other than this low latency retrieval, durable and 'cost is not a factor' all points to the DynamoDB answer here.
9. DynamoDB is typically useful for storing a large number of small records with single digit millisecond latency.
10. DynamoDB. Keywords: indexed data
11. "key = value" and "semi-structured" ==> NoSQL DB = DynamoDB
12. Amazon DynamoDB transactions simplify the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily.
13. Requirements: MOST cost-efficient change to support the increased traffic with minimal changes to the application
* Cost effective: only scale up when needed
* No change to application

Scenario:

1. Design ride sharing application. 
 - App needs consistent and single digit millisecond latency.
 - App must integrate with highly scalable and fully managed database service to track GPS coordinate and user data for all rides.
 - Database service to meet these requirements?
       - **Amazon DynamoDB**

2. Review application that writes data to an Amazon DynamoDB table on daily basis. 
 - Random table read occurs many times per second. 
 - Company wants to allow thousands of low-latency reads and avoid any negative impact to the rest of the application. 
 - What should SA do to meet companys goals
       - **Use DynamoDB Accelerator to cache reads.**

3. Three tiered applications are created to host small news articles.
 - App is expected to serve millions of users.
 - When breaking news occurs, site must handle very large spikes in traffic without impacting database performance. 
 - What design meets these requirements while minimizing costs? 
       - **Use Amazon DynamoDB Accelerator (DAX) to cache read operations to the database**
  
4. Company storing data in DynamoDB table and needs to take daily backups and retain them for 6 months. 
 - How can SA meet these requirements without impacting production workload?
    - **Use Amazon CloudWatch Events to trigger an AWS Lambda function that makes an on-demand backup of the table**

5. Company plans to deploy new application in AWS that reads and writes information to database.
 - Company wants to deploy the application in two different AWS regions in an active active configuration.
 - Database need to replicate to keep information in sync. 
 - What should be used to meet these requirements?
       - **Amazon DynamoDB with global tables**

6. App is scanning an Amazon DynamoDB table that was created with default settings.
 - App occasionally reads stale data when it queries the table. How can this issue be solved?
     - **Update the application to use strongly consistent reads.**

7. SA must design a DynamoDB table to store data about customer activities. 
 - The data is used to analyze recent customer behavior, so data that is less than a week old is heavily accessed and older data is accessed infrequently.
 - Data that is more than one month old never needs to be referenced by the application, but needs to be archived for year end analytics.
 - What is the most cost efficient way to meet these requirements?
       - **Create separate tables for each week's data with higher throughput for the current week.**
       - **Export the old table data from DynamoDB to Amazon S3 using AWS Data Pipeline, and delete the old table.**

8. Company has a Node.js application running on EC2 that currently retrieves data for customers from DynamoDB table.
 - Company is seeing many repeat queries for the same items, and the number of queries is continuing to increase as the application gains popularity.
 - What solution will reduce the number of read capacity units (RCUs) required while minimizing the amount of refactoring that must be done to the application?
     - **Use Amazon DynamoDB Accelerator (DAX) to provide a caching layer**

9. Company is rolling out a new web service but is unsure how many customers service will attract.
 - However, the company is unwilling to accept any downtime.
 - What could SA recommend to the company in order to keep track of customers current session data?
     - **Amazon DynamoDB**

10. University is running an internal web app on AWS that students can access from university n/w to check their exam results. The web app runs on EC2 instances and pulls results from DynamoDB table. Auto scaling is currently configured to add a new web server when CPU is greater than 80% for 5 mins. DynamoDB is config to increased both read and write capacity units by 5 when utilization is greater than 80% Exam results are released at 9am each monday, and 80% of students, attempt to access their unique results within first 30 mins. Despite auto scaling being enabled, students are complaining of slow response times and errors when they view the site. There are no performance complaints after 930am on monday. Which rec should SA make to improve performance in a cost-effective manner?
       - **Use a scheduled job to scale out EC2 before 9:00 a.m. on Monday and to scale down after 9:30 a.m.**

11. Company is looking for fully managed solution to store its players state info for rapidly growing game. App runs on multiple EC2 node, which can scale according to incoming traffic. Request can be routed to any of the nodes, therefore, state info must be stored in centralized DB. The players state info needs to be read with strong consistency and needs conditional updates for any changes. Which service would be MOST cost-effective and scale seamlessly?
       - **Amazon DynamoDB**

12. Customer owns MySQL DB that is accessed by various clients who expect at most, 100ms latency on requests. Once a record is stored in the DB, it rarely changed. Clients only access one record at a time. Database access has been increasing due to increased client demand. The resultant load will soon exceed capacity of most expensive hardware available for purchase. The customer wants to migrate to AWS, and is willing to change DB systems. Which service would alleviate db load issue and offer virtually unlimited scalability for the future?
       - **Amazon DynamoDB**

13. Company wants to durably store data in 8KB chunks. The company will access the data once every few months. However, when the company does access the data, it must be done with as little latency as possible. Which AWS service should SA recommend if cost is NOT a factor?
       - **Amazon DynamoDB**

14. SA is designing a solution to monitor weather changes by the minute. The frontend application is hosted on EC2 instance. The backend must be scalable to a virtually unlimited size, and data retrieval must occur with minimal latency. Which AWS service should the architect use to store the data and achieve these requirements?
    - **Amazon DynamoDB**

15. SA is building a new feature using Lambda to create metadata when a user uploads pic to S3. All metadata must be indexed. Which AWS service should SA use to store this metadata?
       - **Amazon DynamoDB**

16. Org is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which one of the following services MOST effectively meets their needs?
       - **Amazon DynamoDB**

17. Company is launching an app that it expects to be very popular. The company needs a DB that can scale with the rest of the app. The schema will change frequently. The app cannot afford any downtime for DB changes. Which AWS service allows the company to achieve these objectives?
       - **Amazon DynamoDB**

18. Data analytics startup company asks SA to recommend AWS data store options for indexed data. Data processing engine will generate and input more than 64 TB of processed data every day, with item sizes reaching up to 300 KB. The startup is flexible with data storage and is more interested in DB that requires minimal effort to scale with growing dataset size. Which AWS data store service should SA recommend?
       - **Amazon DynamoDB**

19. Web app running on EC2 instances writes data synchronously to DynamoDB table configured for 60 write capacity units. During normal operation the app writes 50Kb/s to the tale, but can scale up to 500kb/s during peak hours. The app is currently throttling errors from DynamoDB table during peak hours. What is the MOST cost-efficient change to support increased traffic with min changes to app?
       - **Configure Amazon DynamoDB Auto Scaling to handle the extra demand.**

## More:

1. Fast and flexible noSQL database service for all apps that need consistent, single-digit millisecond latency at any scale.
2. Fully managed DB and supports both document and key value data models.
3. Make it a great fit for mobile, web, gaming, ad-tech, IOT and many other apps.

## Basics:

1. Stored on SSD storage.
2. Spread across 3 geographically distinct data centers.
3. Eventual consistent reads (Default)
4. Strongly consistent reads.

## Eventual consistent reads:

1. Consistency across all copies of data reached in second.
2. Best read performance. Repeating read after short time returns updated data.

## Strongly consistent reads:

1. Returns a result that reflects all writes that received a successful response prior to the read.

